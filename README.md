# Space Invaders Deep Reinforcement Learning

A comprehensive RL portfolio project implementing and comparing state-of-the-art deep reinforcement learning algorithms on Atari Space Invaders.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9.0-EE4C2C.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ† Key Achievement

**561.50 max reward** using PPO - a **282% improvement** over random baseline (146.95)

## ğŸ“Š Results Summary

| Algorithm        | Max Reward | Mean Reward | Improvement | Status |
| ---------------- | ---------- | ----------- | ----------- | ------ |
| Random Baseline  | 146.95     | 146.95      | -           | âœ…     |
| **Basic DQN**    | **506.00** | 318.56      | **+244%**   | âœ…     |
| **Improved DQN** | **364.00** | 306.56      | **+148%**   | âœ…     |
| **PPO** ğŸ†       | **561.50** | **375.43**  | **+282%**   | âœ…     |

## Project Status

- [x] **Phase 1**: Environment Setup & Exploration
- [x] **Phase 2**: Data Collection & Baseline Performance (146.95 reward)
- [x] **Phase 3**: Preprocessing & Feature Engineering
- [x] **Phase 4**: Basic DQN (**506 max reward**, +244%)
- [x] **Phase 5**: Improved DQN (**364 max reward**, +148%)
- [x] **Phase 6**: PPO Algorithm (**561.50 max reward**, +282%) ğŸ†
- [x] **Phase 7**: Hyperparameter Optimization (Optional/Future Work)
- [x] **Phase 8**: Advanced Analysis & Visualization
- [x] **Phase 9**: Model Interpretability
- [x] **Phase 10**: Deployment & Documentation
- [ ] **Phase 11**: Transfer Learning (Bonus/Future)
- [ ] **Phase 12**: Multi-Agent Learning (Bonus/Future)

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/johnpospisil/space-invaders-rl.git
cd space-invaders-rl

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run Pre-trained PPO Agent

```python
from preprocessing import make_atari_env
from ppo_agent import PPOAgent

# Load environment
env = make_atari_env('ALE/SpaceInvaders-v5', frame_stack=4)

# Load trained PPO agent (best performer)
agent = PPOAgent(env.observation_space.shape, env.action_space.n)
agent.load('models/phase6/ppo_final.pt')

# Play episode
obs, _ = env.reset()
total_reward = 0
done = False

while not done:
    action = agent.select_action(obs, training=False)
    obs, reward, done, truncated, _ = env.step(action)
    total_reward += reward
    done = done or truncated

print(f"Episode reward: {total_reward}")
```

## Setup

```bash
pip install -r requirements.txt
```

## Phase 1: Environment Setup & Exploration

Run the initial exploration:

```bash
python src/phase1_environment_setup.py
```

This will:

- Load the Space Invaders environment
- Run a random agent for 5 episodes
- Display environment statistics
- Save sample frames

**Results**: Baseline established with mean reward of 119.0 Â± 79.0

## Phase 2: Data Collection & Baseline Performance

Run comprehensive baseline data collection:

```bash
python src/phase2_baseline_collection.py
```

This will:

- Run random agent for 100 episodes
- Collect detailed performance metrics
- Generate comprehensive statistical analysis
- Create visualization dashboards
- Save baseline data for comparison

**Results**:

- Mean Reward: 146.95 Â± 93.14
- Mean Episode Length: 504.4 Â± 155.9
- Coefficient of Variation: 0.63 (high variance)

## Phase 3: Preprocessing & Feature Engineering

Run preprocessing demonstration:

```bash
python src/phase3_preprocessing_demo.py
```

This will:

- Demonstrate preprocessing pipeline steps
- Compare raw vs preprocessed observations
- Visualize frame stacking and transformations
- Test preprocessed environment
- Create reusable preprocessing utilities

**Key Features**:

- 72% reduction in pixel dimensions (100,800 â†’ 28,224)
- Grayscale conversion + resize to 84Ã—84
- Frame stacking (4 frames) for temporal information
- Reward clipping to {-1, 0, 1}
- Normalization to [0, 1]
- Follows DQN Nature paper methodology

**Preprocessing Module**: `src/preprocessing.py` provides reusable wrappers for all phases

## Phase 4: DQN Agent - Part 1 (Basic Implementation)

Train a basic DQN agent:

```bash
python src/phase4_train_dqn.py
```

This will:

- Train DQN agent for 1M steps
- Implement experience replay with 100k buffer
- Use epsilon-greedy exploration (1.0 â†’ 0.01)
- Save checkpoints every 100k steps
- Evaluate agent every 50k steps
- Generate training curves and visualizations

**Architecture**:

- CNN: 3 conv layers (32â†’64â†’64 filters) + 2 FC layers (512â†’n_actions)
- Optimizer: Adam with learning rate 1e-4
- Loss: Huber loss with gradient clipping
- Target network updated every 10k steps
- Follows Nature DQN paper (Mnih et al., 2015)

**Training Parameters**:

- Total steps: 1,000,000
- Batch size: 32
- Replay buffer: 100,000 transitions
- Learning starts: 50,000 steps
- Gamma (discount): 0.99

**Note**: Training will take several hours on CPU, much faster with GPU.

**Analysis**: Use `notebooks/phase4_dqn_training.ipynb` to analyze results

**Phase 4 Results**: Max reward 506 (244% improvement over baseline!)

## Phase 5: DQN Improvements

Train improved DQN with multiple enhancements:

```bash
python src/phase5_train_improved_dqn.py
```

Or for quick test (~30 min):

```bash
python run_phase5_quick_test.py
```

**Improvements**:

1. **Double DQN**: Reduces Q-value overestimation by using online network for action selection
2. **Dueling Networks**: Separates value function V(s) and advantage function A(s,a)
3. **Prioritized Experience Replay**: Samples important transitions more frequently

**Architecture**:

- Dueling network with separate value and advantage streams
- Same CNN backbone as Phase 4
- Prioritized replay with Î±=0.6, Î² annealing from 0.4 to 1.0
- All Phase 4 hyperparameters maintained for fair comparison

**Expected Results**: 10-30% improvement over Phase 4 (~560-660 reward)

**Actual Results**: Max reward 364 (+148% over baseline). More stable than Basic DQN but lower peak performance.

## Phase 6: PPO (Proximal Policy Optimization) ğŸ†

Train PPO agent (policy-gradient method):

```bash
python src/phase6_train_ppo.py
```

Or for quick test (~30 min):

```bash
python run_phase6_quick_test.py
```

**Algorithm**: Proximal Policy Optimization (policy-gradient approach)

**Key Differences from DQN**:

- **Learning approach**: Direct policy optimization vs Q-value learning
- **Exploration**: Stochastic policy vs Îµ-greedy
- **Updates**: On-policy (recent data) vs off-policy (replay buffer)
- **Stability**: Clipped objective prevents large updates

**Architecture**:

- Actor-Critic with shared CNN backbone
- Actor head: Outputs policy Ï€(a|s)
- Critic head: Outputs value V(s)
- Generalized Advantage Estimation (GAE, Î»=0.95)

**Hyperparameters**:

- Learning rate: 3e-4
- Rollout steps: 2,048
- Mini-batch size: 256
- Update epochs: 4 per rollout
- Clip epsilon: 0.2
- Gamma: 0.99

**Results**: **561.50 max reward** - BEST PERFORMANCE! (+282% over baseline, beats all DQN variants)

## Phase 8: Advanced Analysis & Visualization

Comprehensive comparison of all algorithms:

```bash
python src/phase8_advanced_analysis.py
```

**Analyses**:

- Learning curve comparison across all algorithms
- Statistical significance testing
- Performance distributions
- Sample efficiency analysis
- Training stability metrics

**Key Findings**:

- PPO achieves highest max reward (561.50)
- Basic DQN surprisingly outperforms Improved DQN (506 vs 364)
- Improved DQN most stable (lowest std dev: 49.20)
- All algorithms significantly beat baseline (>140% improvement)

**Generated Files**:

- `outputs/phase8/comprehensive_analysis.png` - Multi-panel visualization
- `data/phase8/statistical_analysis.txt` - Statistical report
- `data/phase8/analysis_summary.json` - Machine-readable results

## Phase 9: Model Interpretability

Understand what the agent learned:

```bash
python src/phase9_interpretability.py
```

**Analyses**:

1. **Saliency Maps**: Gradient-based visualization showing which pixels influence decisions
2. **CNN Activations**: What features each convolutional layer detects
3. **Policy Behavior**: Action preference analysis over 20 episodes

**Key Findings**:

- **Action Preferences** (from 20 episodes):
  - RIGHTFIRE: 67.9% (primary offensive strategy - move right while shooting)
  - LEFTFIRE: 11.8% (secondary offensive strategy)
  - RIGHT: 9.5% (repositioning)
  - LEFT: 6.5% (repositioning)
  - NOOP: 3.9% (minimal idle time)
  - FIRE: 0.5% (rarely fires while stationary)
- **Mean Episode Performance**: 393.00 Â± 137.32 reward
- **Mean Episode Length**: 165.4 Â± 37.3 steps
- **Strategy**: Aggressive offensive (79.7% time spent firing while moving)
- **Saliency Focus**: Agent attends to enemy positions, projectiles, and player ship

**Generated Files**:

- `outputs/phase9/saliency_maps/saliency_analysis.png` - Attention visualization
- `outputs/phase9/activations/cnn_activations.png` - Feature detection
- `outputs/phase9/policy_analysis/policy_behavior.png` - Action distribution
- `outputs/phase9/key_moments.png` - Critical decision points

## Expected Results\*\*: 10-30% improvement over Phase 4 (~560-660 reward)

## Project Structure

```
space_invaders_rl/
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ preprocessing.py              # Image preprocessing pipeline
â”‚   â”œâ”€â”€ replay_buffer.py             # Experience replay
â”‚   â”œâ”€â”€ dqn_agent.py                 # Basic DQN
â”‚   â”œâ”€â”€ prioritized_replay.py        # Prioritized experience replay
â”‚   â”œâ”€â”€ dueling_dqn.py               # Improved DQN
â”‚   â”œâ”€â”€ ppo_agent.py                 # PPO implementation
â”‚   â”œâ”€â”€ phase4_train_dqn.py          # DQN training
â”‚   â”œâ”€â”€ phase5_train_improved_dqn.py # Improved DQN training
â”‚   â”œâ”€â”€ phase6_train_ppo.py          # PPO training
â”‚   â”œâ”€â”€ phase8_advanced_analysis.py  # Comparative analysis
â”‚   â””â”€â”€ phase9_interpretability.py   # Model interpretability
â”œâ”€â”€ models/                           # Trained models
â”‚   â”œâ”€â”€ phase4/dqn_final.pt          # Trained DQN (506 reward)
â”‚   â”œâ”€â”€ phase5/improved_dqn_final.pt # Trained Improved DQN (364 reward)
â”‚   â””â”€â”€ phase6/ppo_final.pt          # Trained PPO (561.50 reward) ğŸ†
â”œâ”€â”€ data/                             # Training metrics & results
â”œâ”€â”€ outputs/                          # Visualizations
â”‚   â”œâ”€â”€ phase8/comprehensive_analysis.png  # Algorithm comparison
â”‚   â””â”€â”€ phase9/                       # Interpretability visualizations
â”œâ”€â”€ docs/                             # Detailed documentation
â””â”€â”€ requirements.txt                  # Dependencies
```

## ğŸ”¬ Technical Details

### Algorithms Implemented

**1. Basic DQN (Deep Q-Network)**

- Experience replay buffer (100k capacity)
- Target network (updated every 10k steps)
- Îµ-greedy exploration (1.0 â†’ 0.01)
- Huber loss with gradient clipping
- **Result**: 506 max reward

**2. Improved DQN**

- **Double DQN**: Reduces Q-value overestimation
- **Dueling Networks**: Separates V(s) and A(s,a) streams
- **Prioritized Experience Replay**: Samples important transitions (Î±=0.6, Î²=0.4â†’1.0)
- **Result**: 364 max reward (more stable, lower peak)

**3. PPO (Proximal Policy Optimization)** ğŸ†

- Actor-Critic architecture with shared CNN
- Generalized Advantage Estimation (GAE, Î»=0.95)
- Clipped surrogate objective (Îµ=0.2)
- On-policy learning with rollout collection
- **Result**: 561.50 max reward (BEST)

### Preprocessing Pipeline

- Grayscale conversion (210Ã—160Ã—3 â†’ 210Ã—160)
- Resize to 84Ã—84 (-72% dimensions)
- Frame stacking (4 frames) for temporal information
- Reward clipping {-1, 0, +1}
- Normalization [0, 1]

### Network Architecture

**CNN Backbone** (shared by all algorithms):

```
Conv2D(4, 32, kernel=8, stride=4) + ReLU
Conv2D(32, 64, kernel=4, stride=2) + ReLU
Conv2D(64, 64, kernel=3, stride=1) + ReLU
Flatten + FC(3136, 512) + ReLU
```

**DQN Output**: FC(512, 6) â†’ Q(s,a) for each action

**Dueling DQN Output**:

- Value stream: FC(512, 1) â†’ V(s)
- Advantage stream: FC(512, 6) â†’ A(s,a)
- Combined: Q(s,a) = V(s) + (A(s,a) - mean(A))

**PPO Output**:

- Actor head: FC(512, 6) + Softmax â†’ Ï€(a|s)
- Critic head: FC(512, 1) â†’ V(s)

## ğŸ“š Documentation

- [Phase 3: Preprocessing Pipeline](docs/phase3_preprocessing.md)
- [Phase 5: Improved DQN Guide](docs/PHASE5_GUIDE.md)
- [Phase 6: PPO Implementation](docs/phase6_ppo.md)
- [Phase 7: Hyperparameter Optimization](docs/phase7_hyperparameter_optimization.md)

## ğŸ“ Key Learnings

### Why PPO Outperformed DQN

1. **Natural Exploration**: Stochastic policy vs Îµ-greedy
2. **Trust Region Updates**: Clipped objective prevents destructive changes
3. **Direct Policy Optimization**: No Q-value estimation errors
4. **Advantage Estimation**: GAE reduces variance in gradients

### Why Basic DQN Beat Improved DQN

Surprising finding - basic DQN (506) outperformed improved version (364):

- Prioritized replay may have over-sampled rare events
- Dueling architecture may need more training time
- Simpler can be better with limited compute
- Demonstrates importance of empirical validation

## ğŸš€ Performance Tips

**For Training**:

- Use GPU if available (10x faster)
- Start with quick tests before full training
- Monitor eval rewards, not training rewards
- Save checkpoints frequently

**For Inference**:

- Load PPO model for best performance
- Use `training=False` for deterministic actions
- Frame stacking is critical - don't skip preprocessing

## Project Structure

```
space_invaders_rl/
â”œâ”€â”€ src/                    # Source code
â”œâ”€â”€ notebooks/              # Jupyter notebooks for analysis
â”œâ”€â”€ data/                   # Data and logs
â”œâ”€â”€ models/                 # Saved models
â”œâ”€â”€ outputs/                # Visualizations and results
â””â”€â”€ requirements.txt        # Dependencies
```

## Author

Data Science Portfolio Project - 2025
