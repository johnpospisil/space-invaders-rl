{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91b6c51",
   "metadata": {},
   "source": [
    "# Phase 3: Preprocessing & Feature Engineering\n",
    "\n",
    "This notebook explores the preprocessing pipeline that transforms raw Atari frames into a format suitable for deep reinforcement learning.\n",
    "\n",
    "## Objectives\n",
    "1. Understand each preprocessing step\n",
    "2. Compare raw vs preprocessed observations\n",
    "3. Visualize the transformation pipeline\n",
    "4. Test preprocessed environment\n",
    "5. Validate preprocessing benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from preprocessing import make_atari_env, WarpFrame, FrameStack, ClipRewardEnv\n",
    "\n",
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(\"âœ“ Preprocessing module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0ce69",
   "metadata": {},
   "source": [
    "## 1. Raw vs Preprocessed Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create raw environment\n",
    "env_raw = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array')\n",
    "obs_raw, _ = env_raw.reset()\n",
    "\n",
    "# Create preprocessed environment\n",
    "env_preprocessed = make_atari_env('ALE/SpaceInvaders-v5', frame_stack=4, clip_rewards=True)\n",
    "obs_preprocessed, _ = env_preprocessed.reset()\n",
    "\n",
    "print(\"OBSERVATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRaw Observation:\")\n",
    "print(f\"  Shape: {obs_raw.shape}\")\n",
    "print(f\"  Dtype: {obs_raw.dtype}\")\n",
    "print(f\"  Memory: {obs_raw.nbytes / 1024:.2f} KB\")\n",
    "print(f\"  Range: [{obs_raw.min()}, {obs_raw.max()}]\")\n",
    "\n",
    "print(f\"\\nPreprocessed Observation:\")\n",
    "print(f\"  Shape: {obs_preprocessed.shape}\")\n",
    "print(f\"  Dtype: {obs_preprocessed.dtype}\")\n",
    "print(f\"  Memory: {obs_preprocessed.nbytes / 1024:.2f} KB\")\n",
    "print(f\"  Range: [{obs_preprocessed.min():.2f}, {obs_preprocessed.max():.2f}]\")\n",
    "\n",
    "# Calculate reductions\n",
    "memory_reduction = (1 - obs_preprocessed.nbytes / obs_raw.nbytes) * 100\n",
    "raw_dims = np.prod(obs_raw.shape)\n",
    "proc_dims = np.prod(obs_preprocessed.shape)\n",
    "dim_reduction = (1 - proc_dims / raw_dims) * 100\n",
    "\n",
    "print(f\"\\nðŸ’¾ REDUCTION:\")\n",
    "print(f\"  Memory: {memory_reduction:.1f}%\")\n",
    "print(f\"  Dimensions: {raw_dims:,} â†’ {proc_dims:,} ({dim_reduction:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad20b0",
   "metadata": {},
   "source": [
    "## 2. Visualize Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a more interesting frame\n",
    "env = gym.make('ALE/SpaceInvaders-v5', render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "for _ in range(30):\n",
    "    obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# Apply each preprocessing step\n",
    "obs_gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "obs_resized = cv2.resize(obs_gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "obs_normalized = obs_resized.astype(np.float32) / 255.0\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(obs)\n",
    "axes[0, 0].set_title(f'Step 1: Raw RGB\\nShape: {obs.shape}', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(obs_gray, cmap='gray')\n",
    "axes[0, 1].set_title(f'Step 2: Grayscale\\nShape: {obs_gray.shape}', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(obs_resized, cmap='gray')\n",
    "axes[1, 0].set_title(f'Step 3: Resized to 84Ã—84\\nShape: {obs_resized.shape}', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(obs_normalized, cmap='gray', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title(f'Step 4: Normalized [0,1]\\nShape: {obs_normalized.shape}', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle('Preprocessing Pipeline', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077409d",
   "metadata": {},
   "source": [
    "## 3. Frame Stacking Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86918f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with frame stacking\n",
    "env_stacked = make_atari_env('ALE/SpaceInvaders-v5', frame_stack=4)\n",
    "obs_stacked, _ = env_stacked.reset()\n",
    "\n",
    "# Take some actions to populate frame stack\n",
    "for _ in range(30):\n",
    "    obs_stacked, _, _, _, _ = env_stacked.step(env_stacked.action_space.sample())\n",
    "\n",
    "print(f\"Frame Stack Shape: {obs_stacked.shape}\")\n",
    "print(f\"This represents 4 consecutive frames stacked along the channel dimension\")\n",
    "\n",
    "# Visualize all 4 frames\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].imshow(obs_stacked[:, :, i], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[i].set_title(f'Frame {i+1} (t-{3-i})', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Frame Stack: 4 Consecutive Frames', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Frame stacking captures motion:\")\n",
    "print(\"   - Frame 1 (t-3): oldest frame\")\n",
    "print(\"   - Frame 4 (t-0): current frame\")\n",
    "print(\"   - Agent can infer velocity and direction from multiple frames\")\n",
    "\n",
    "env_stacked.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08cc9d",
   "metadata": {},
   "source": [
    "## 4. Reward Clipping Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a09535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reward clipping\n",
    "print(\"REWARD CLIPPING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample rewards and their clipped versions\n",
    "sample_rewards = [0, 5, 10, 15, 20, 40, 100, -5, -10]\n",
    "clipped_rewards = [np.sign(r) for r in sample_rewards]\n",
    "\n",
    "print(\"\\nOriginal â†’ Clipped\")\n",
    "print(\"-\" * 30)\n",
    "for orig, clipped in zip(sample_rewards, clipped_rewards):\n",
    "    print(f\"  {orig:4d} â†’ {clipped:2.0f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Benefits of Reward Clipping:\")\n",
    "print(\"   âœ“ Reduces variance in gradients\")\n",
    "print(\"   âœ“ Makes learning more stable\")\n",
    "print(\"   âœ“ Treats all positive rewards equally\")\n",
    "print(\"   âœ“ Standard practice for DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f4788",
   "metadata": {},
   "source": [
    "## 5. Load and Analyze Preprocessing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0163fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing info\n",
    "with open('../data/phase3/preprocessing_info.json', 'r') as f:\n",
    "    preprocessing_info = json.load(f)\n",
    "\n",
    "print(\"PREPROCESSING PIPELINE STEPS\")\n",
    "print(\"=\" * 60)\n",
    "for step, description in preprocessing_info['preprocessing_steps'].items():\n",
    "    step_num = step.split('_')[0]\n",
    "    step_name = ' '.join(step.split('_')[1:]).title()\n",
    "    print(f\"\\n{step_num}. {step_name}\")\n",
    "    print(f\"   {description}\")\n",
    "\n",
    "print(\"\\n\\nKEY BENEFITS\")\n",
    "print(\"=\" * 60)\n",
    "for benefit, description in preprocessing_info['benefits'].items():\n",
    "    print(f\"âœ“ {benefit.replace('_', ' ').title()}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cbec24",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a06f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance stats\n",
    "perf_stats = preprocessing_info['performance_stats']\n",
    "\n",
    "print(\"PREPROCESSED ENVIRONMENT PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Episodes Tested: {perf_stats['num_episodes']}\")\n",
    "print(f\"Mean Reward (clipped): {perf_stats['mean_reward']:.2f} Â± {perf_stats['std_reward']:.2f}\")\n",
    "print(f\"Mean Episode Length: {perf_stats['mean_length']:.1f} Â± {perf_stats['std_length']:.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "episodes = range(1, len(perf_stats['episode_rewards']) + 1)\n",
    "\n",
    "# Rewards\n",
    "axes[0].plot(episodes, perf_stats['episode_rewards'], 'o-', alpha=0.6, color='steelblue')\n",
    "axes[0].axhline(y=perf_stats['mean_reward'], color='red', linestyle='--', \n",
    "                label=f\"Mean: {perf_stats['mean_reward']:.2f}\", linewidth=2)\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Total Reward (Clipped)', fontsize=12)\n",
    "axes[0].set_title('Preprocessed Environment - Episode Rewards', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lengths\n",
    "axes[1].bar(episodes, perf_stats['episode_lengths'], alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].axhline(y=perf_stats['mean_length'], color='red', linestyle='--', \n",
    "                label=f\"Mean: {perf_stats['mean_length']:.1f}\", linewidth=2)\n",
    "axes[1].set_xlabel('Episode', fontsize=12)\n",
    "axes[1].set_ylabel('Episode Length', fontsize=12)\n",
    "axes[1].set_title('Preprocessed Environment - Episode Lengths', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9e7f1",
   "metadata": {},
   "source": [
    "## 7. Key Insights\n",
    "\n",
    "### Preprocessing Achievements:\n",
    "\n",
    "1. **Dimensionality Reduction**: 72% reduction in pixel dimensions (100,800 â†’ 28,224)\n",
    "2. **Memory Efficiency**: Reduced memory footprint per observation\n",
    "3. **Temporal Information**: Frame stacking captures motion and velocity\n",
    "4. **Training Stability**: Reward clipping reduces gradient variance\n",
    "5. **Standardization**: Follows DQN Nature paper methodology\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- âœ… **Faster Training**: Smaller observations = faster neural network forward/backward passes\n",
    "- âœ… **Better Learning**: Frame stacking provides temporal context\n",
    "- âœ… **Stability**: Reward clipping prevents gradient explosion\n",
    "- âœ… **Reproducibility**: Standard preprocessing allows fair comparisons\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "With preprocessing pipeline complete, we're ready to implement our first DQN agent in Phase 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a548d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Phase 3 Complete!\n",
    "\n",
    "We have:\n",
    "- Implemented comprehensive preprocessing pipeline\n",
    "- Reduced observation dimensions by 72%\n",
    "- Added temporal information via frame stacking\n",
    "- Implemented reward clipping for stability\n",
    "- Created reusable preprocessing utilities\n",
    "- Validated preprocessing with random agent\n",
    "\n",
    "**Ready for Phase 4: DQN Agent - Part 1 (Basic Implementation)**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
